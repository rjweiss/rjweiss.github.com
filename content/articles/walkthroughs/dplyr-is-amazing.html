<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta http-equiv="x-ua-compatible" content="IE=9" >

<title>The data</title>

<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 12px;
   margin: 8px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 { 
   font-size:2.2em; 
}

h2 { 
   font-size:1.8em; 
}

h3 { 
   font-size:1.4em; 
}

h4 { 
   font-size:1.0em; 
}

h5 { 
   font-size:0.9em; 
}

h6 { 
   font-size:0.8em; 
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre {	
   margin-top: 0;
   max-width: 95%;
   border: 1px solid #ccc;
   white-space: pre-wrap;
}

pre code {
   display: block; padding: 0.5em;
}

code.r, code.cpp {
   background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * { 
      background: transparent !important; 
      color: black !important; 
      filter:none !important; 
      -ms-filter: none !important; 
   }

   body { 
      font-size:12pt; 
      max-width:100%; 
   }
       
   a, a:visited { 
      text-decoration: underline; 
   }

   hr { 
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote { 
      padding-right: 1em; 
      page-break-inside: avoid; 
   }

   tr, img { 
      page-break-inside: avoid; 
   }

   img { 
      max-width: 100% !important; 
   }

   @page :left { 
      margin: 15mm 20mm 15mm 10mm; 
   }
     
   @page :right { 
      margin: 15mm 10mm 15mm 20mm; 
   }

   p, h2, h3 { 
      orphans: 3; widows: 3; 
   }

   h2, h3 { 
      page-break-after: avoid; 
   }
}

</style>

<!-- Styles for R syntax highlighter -->
<style type="text/css">
   pre .operator,
   pre .paren {
     color: rgb(104, 118, 135)
   }

   pre .literal {
     color: rgb(88, 72, 246)
   }

   pre .number {
     color: rgb(0, 0, 205);
   }

   pre .comment {
     color: rgb(76, 136, 107);
   }

   pre .keyword {
     color: rgb(0, 0, 255);
   }

   pre .identifier {
     color: rgb(0, 0, 0);
   }

   pre .string {
     color: rgb(3, 106, 7);
   }
</style>

<!-- R syntax highlighter -->
<script type="text/javascript">
var hljs=new function(){function m(p){return p.replace(/&/gm,"&amp;").replace(/</gm,"&lt;")}function f(r,q,p){return RegExp(q,"m"+(r.cI?"i":"")+(p?"g":""))}function b(r){for(var p=0;p<r.childNodes.length;p++){var q=r.childNodes[p];if(q.nodeName=="CODE"){return q}if(!(q.nodeType==3&&q.nodeValue.match(/\s+/))){break}}}function h(t,s){var p="";for(var r=0;r<t.childNodes.length;r++){if(t.childNodes[r].nodeType==3){var q=t.childNodes[r].nodeValue;if(s){q=q.replace(/\n/g,"")}p+=q}else{if(t.childNodes[r].nodeName=="BR"){p+="\n"}else{p+=h(t.childNodes[r])}}}if(/MSIE [678]/.test(navigator.userAgent)){p=p.replace(/\r/g,"\n")}return p}function a(s){var r=s.className.split(/\s+/);r=r.concat(s.parentNode.className.split(/\s+/));for(var q=0;q<r.length;q++){var p=r[q].replace(/^language-/,"");if(e[p]){return p}}}function c(q){var p=[];(function(s,t){for(var r=0;r<s.childNodes.length;r++){if(s.childNodes[r].nodeType==3){t+=s.childNodes[r].nodeValue.length}else{if(s.childNodes[r].nodeName=="BR"){t+=1}else{if(s.childNodes[r].nodeType==1){p.push({event:"start",offset:t,node:s.childNodes[r]});t=arguments.callee(s.childNodes[r],t);p.push({event:"stop",offset:t,node:s.childNodes[r]})}}}}return t})(q,0);return p}function k(y,w,x){var q=0;var z="";var s=[];function u(){if(y.length&&w.length){if(y[0].offset!=w[0].offset){return(y[0].offset<w[0].offset)?y:w}else{return w[0].event=="start"?y:w}}else{return y.length?y:w}}function t(D){var A="<"+D.nodeName.toLowerCase();for(var B=0;B<D.attributes.length;B++){var C=D.attributes[B];A+=" "+C.nodeName.toLowerCase();if(C.value!==undefined&&C.value!==false&&C.value!==null){A+='="'+m(C.value)+'"'}}return A+">"}while(y.length||w.length){var v=u().splice(0,1)[0];z+=m(x.substr(q,v.offset-q));q=v.offset;if(v.event=="start"){z+=t(v.node);s.push(v.node)}else{if(v.event=="stop"){var p,r=s.length;do{r--;p=s[r];z+=("</"+p.nodeName.toLowerCase()+">")}while(p!=v.node);s.splice(r,1);while(r<s.length){z+=t(s[r]);r++}}}}return z+m(x.substr(q))}function j(){function q(x,y,v){if(x.compiled){return}var u;var s=[];if(x.k){x.lR=f(y,x.l||hljs.IR,true);for(var w in x.k){if(!x.k.hasOwnProperty(w)){continue}if(x.k[w] instanceof Object){u=x.k[w]}else{u=x.k;w="keyword"}for(var r in u){if(!u.hasOwnProperty(r)){continue}x.k[r]=[w,u[r]];s.push(r)}}}if(!v){if(x.bWK){x.b="\\b("+s.join("|")+")\\s"}x.bR=f(y,x.b?x.b:"\\B|\\b");if(!x.e&&!x.eW){x.e="\\B|\\b"}if(x.e){x.eR=f(y,x.e)}}if(x.i){x.iR=f(y,x.i)}if(x.r===undefined){x.r=1}if(!x.c){x.c=[]}x.compiled=true;for(var t=0;t<x.c.length;t++){if(x.c[t]=="self"){x.c[t]=x}q(x.c[t],y,false)}if(x.starts){q(x.starts,y,false)}}for(var p in e){if(!e.hasOwnProperty(p)){continue}q(e[p].dM,e[p],true)}}function d(B,C){if(!j.called){j();j.called=true}function q(r,M){for(var L=0;L<M.c.length;L++){if((M.c[L].bR.exec(r)||[null])[0]==r){return M.c[L]}}}function v(L,r){if(D[L].e&&D[L].eR.test(r)){return 1}if(D[L].eW){var M=v(L-1,r);return M?M+1:0}return 0}function w(r,L){return L.i&&L.iR.test(r)}function K(N,O){var M=[];for(var L=0;L<N.c.length;L++){M.push(N.c[L].b)}var r=D.length-1;do{if(D[r].e){M.push(D[r].e)}r--}while(D[r+1].eW);if(N.i){M.push(N.i)}return f(O,M.join("|"),true)}function p(M,L){var N=D[D.length-1];if(!N.t){N.t=K(N,E)}N.t.lastIndex=L;var r=N.t.exec(M);return r?[M.substr(L,r.index-L),r[0],false]:[M.substr(L),"",true]}function z(N,r){var L=E.cI?r[0].toLowerCase():r[0];var M=N.k[L];if(M&&M instanceof Array){return M}return false}function F(L,P){L=m(L);if(!P.k){return L}var r="";var O=0;P.lR.lastIndex=0;var M=P.lR.exec(L);while(M){r+=L.substr(O,M.index-O);var N=z(P,M);if(N){x+=N[1];r+='<span class="'+N[0]+'">'+M[0]+"</span>"}else{r+=M[0]}O=P.lR.lastIndex;M=P.lR.exec(L)}return r+L.substr(O,L.length-O)}function J(L,M){if(M.sL&&e[M.sL]){var r=d(M.sL,L);x+=r.keyword_count;return r.value}else{return F(L,M)}}function I(M,r){var L=M.cN?'<span class="'+M.cN+'">':"";if(M.rB){y+=L;M.buffer=""}else{if(M.eB){y+=m(r)+L;M.buffer=""}else{y+=L;M.buffer=r}}D.push(M);A+=M.r}function G(N,M,Q){var R=D[D.length-1];if(Q){y+=J(R.buffer+N,R);return false}var P=q(M,R);if(P){y+=J(R.buffer+N,R);I(P,M);return P.rB}var L=v(D.length-1,M);if(L){var O=R.cN?"</span>":"";if(R.rE){y+=J(R.buffer+N,R)+O}else{if(R.eE){y+=J(R.buffer+N,R)+O+m(M)}else{y+=J(R.buffer+N+M,R)+O}}while(L>1){O=D[D.length-2].cN?"</span>":"";y+=O;L--;D.length--}var r=D[D.length-1];D.length--;D[D.length-1].buffer="";if(r.starts){I(r.starts,"")}return R.rE}if(w(M,R)){throw"Illegal"}}var E=e[B];var D=[E.dM];var A=0;var x=0;var y="";try{var s,u=0;E.dM.buffer="";do{s=p(C,u);var t=G(s[0],s[1],s[2]);u+=s[0].length;if(!t){u+=s[1].length}}while(!s[2]);if(D.length>1){throw"Illegal"}return{r:A,keyword_count:x,value:y}}catch(H){if(H=="Illegal"){return{r:0,keyword_count:0,value:m(C)}}else{throw H}}}function g(t){var p={keyword_count:0,r:0,value:m(t)};var r=p;for(var q in e){if(!e.hasOwnProperty(q)){continue}var s=d(q,t);s.language=q;if(s.keyword_count+s.r>r.keyword_count+r.r){r=s}if(s.keyword_count+s.r>p.keyword_count+p.r){r=p;p=s}}if(r.language){p.second_best=r}return p}function i(r,q,p){if(q){r=r.replace(/^((<[^>]+>|\t)+)/gm,function(t,w,v,u){return w.replace(/\t/g,q)})}if(p){r=r.replace(/\n/g,"<br>")}return r}function n(t,w,r){var x=h(t,r);var v=a(t);var y,s;if(v){y=d(v,x)}else{return}var q=c(t);if(q.length){s=document.createElement("pre");s.innerHTML=y.value;y.value=k(q,c(s),x)}y.value=i(y.value,w,r);var u=t.className;if(!u.match("(\\s|^)(language-)?"+v+"(\\s|$)")){u=u?(u+" "+v):v}if(/MSIE [678]/.test(navigator.userAgent)&&t.tagName=="CODE"&&t.parentNode.tagName=="PRE"){s=t.parentNode;var p=document.createElement("div");p.innerHTML="<pre><code>"+y.value+"</code></pre>";t=p.firstChild.firstChild;p.firstChild.cN=s.cN;s.parentNode.replaceChild(p.firstChild,s)}else{t.innerHTML=y.value}t.className=u;t.result={language:v,kw:y.keyword_count,re:y.r};if(y.second_best){t.second_best={language:y.second_best.language,kw:y.second_best.keyword_count,re:y.second_best.r}}}function o(){if(o.called){return}o.called=true;var r=document.getElementsByTagName("pre");for(var p=0;p<r.length;p++){var q=b(r[p]);if(q){n(q,hljs.tabReplace)}}}function l(){if(window.addEventListener){window.addEventListener("DOMContentLoaded",o,false);window.addEventListener("load",o,false)}else{if(window.attachEvent){window.attachEvent("onload",o)}else{window.onload=o}}}var e={};this.LANGUAGES=e;this.highlight=d;this.highlightAuto=g;this.fixMarkup=i;this.highlightBlock=n;this.initHighlighting=o;this.initHighlightingOnLoad=l;this.IR="[a-zA-Z][a-zA-Z0-9_]*";this.UIR="[a-zA-Z_][a-zA-Z0-9_]*";this.NR="\\b\\d+(\\.\\d+)?";this.CNR="\\b(0[xX][a-fA-F0-9]+|(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)";this.BNR="\\b(0b[01]+)";this.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|\\.|-|-=|/|/=|:|;|<|<<|<<=|<=|=|==|===|>|>=|>>|>>=|>>>|>>>=|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~";this.ER="(?![\\s\\S])";this.BE={b:"\\\\.",r:0};this.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[this.BE],r:0};this.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[this.BE],r:0};this.CLCM={cN:"comment",b:"//",e:"$"};this.CBLCLM={cN:"comment",b:"/\\*",e:"\\*/"};this.HCM={cN:"comment",b:"#",e:"$"};this.NM={cN:"number",b:this.NR,r:0};this.CNM={cN:"number",b:this.CNR,r:0};this.BNM={cN:"number",b:this.BNR,r:0};this.inherit=function(r,s){var p={};for(var q in r){p[q]=r[q]}if(s){for(var q in s){p[q]=s[q]}}return p}}();hljs.LANGUAGES.cpp=function(){var a={keyword:{"false":1,"int":1,"float":1,"while":1,"private":1,"char":1,"catch":1,"export":1,virtual:1,operator:2,sizeof:2,dynamic_cast:2,typedef:2,const_cast:2,"const":1,struct:1,"for":1,static_cast:2,union:1,namespace:1,unsigned:1,"long":1,"throw":1,"volatile":2,"static":1,"protected":1,bool:1,template:1,mutable:1,"if":1,"public":1,friend:2,"do":1,"return":1,"goto":1,auto:1,"void":2,"enum":1,"else":1,"break":1,"new":1,extern:1,using:1,"true":1,"class":1,asm:1,"case":1,typeid:1,"short":1,reinterpret_cast:2,"default":1,"double":1,register:1,explicit:1,signed:1,typename:1,"try":1,"this":1,"switch":1,"continue":1,wchar_t:1,inline:1,"delete":1,alignof:1,char16_t:1,char32_t:1,constexpr:1,decltype:1,noexcept:1,nullptr:1,static_assert:1,thread_local:1,restrict:1,_Bool:1,complex:1},built_in:{std:1,string:1,cin:1,cout:1,cerr:1,clog:1,stringstream:1,istringstream:1,ostringstream:1,auto_ptr:1,deque:1,list:1,queue:1,stack:1,vector:1,map:1,set:1,bitset:1,multiset:1,multimap:1,unordered_set:1,unordered_map:1,unordered_multiset:1,unordered_multimap:1,array:1,shared_ptr:1}};return{dM:{k:a,i:"</",c:[hljs.CLCM,hljs.CBLCLM,hljs.QSM,{cN:"string",b:"'\\\\?.",e:"'",i:"."},{cN:"number",b:"\\b(\\d+(\\.\\d*)?|\\.\\d+)(u|U|l|L|ul|UL|f|F)"},hljs.CNM,{cN:"preprocessor",b:"#",e:"$"},{cN:"stl_container",b:"\\b(deque|list|queue|stack|vector|map|set|bitset|multiset|multimap|unordered_map|unordered_set|unordered_multiset|unordered_multimap|array)\\s*<",e:">",k:a,r:10,c:["self"]}]}}}();hljs.LANGUAGES.r={dM:{c:[hljs.HCM,{cN:"number",b:"\\b0[xX][0-9a-fA-F]+[Li]?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+(?:[eE][+\\-]?\\d*)?L\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+\\.(?!\\d)(?:i\\b)?",e:hljs.IMMEDIATE_RE,r:1},{cN:"number",b:"\\b\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"keyword",b:"(?:tryCatch|library|setGeneric|setGroupGeneric)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\.",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\d+(?![\\w.])",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\b(?:function)",e:hljs.IMMEDIATE_RE,r:2},{cN:"keyword",b:"(?:if|in|break|next|repeat|else|for|return|switch|while|try|stop|warning|require|attach|detach|source|setMethod|setClass)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"literal",b:"(?:NA|NA_integer_|NA_real_|NA_character_|NA_complex_)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"literal",b:"(?:NULL|TRUE|FALSE|T|F|Inf|NaN)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"identifier",b:"[a-zA-Z.][a-zA-Z0-9._]*\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"<\\-(?!\\s*\\d)",e:hljs.IMMEDIATE_RE,r:2},{cN:"operator",b:"\\->|<\\-",e:hljs.IMMEDIATE_RE,r:1},{cN:"operator",b:"%%|~",e:hljs.IMMEDIATE_RE},{cN:"operator",b:">=|<=|==|!=|\\|\\||&&|=|\\+|\\-|\\*|/|\\^|>|<|!|&|\\||\\$|:",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"%",e:"%",i:"\\n",r:1},{cN:"identifier",b:"`",e:"`",r:0},{cN:"string",b:'"',e:'"',c:[hljs.BE],r:0},{cN:"string",b:"'",e:"'",c:[hljs.BE],r:0},{cN:"paren",b:"[[({\\])}]",e:hljs.IMMEDIATE_RE,r:0}]}};
hljs.initHighlightingOnLoad();
</script>




</head>

<body>
<p>Title: <code>dplyr</code> is amazing
Slug: dplyr-is-amazing
Category: Walkthroughs
Status: draft
Date: 2014-05-10 15:30</p>

<p>Everyone&#39;s jumping on the big data train.  For <code>R</code> users, &ldquo;Big Data&rdquo; usually refers to dimensionality: lots of rows, lots of columns.  The sad reality of base <code>R</code> is that it wasn&#39;t really designed for data of that scale.  Luckily, there are lots of people working on making <code>R</code> more capable of handling larger data sets.</p>

<p>I&#39;ve been meaning to test out <code>dply</code> against <code>plyr</code> for some time now.  Here&#39;s a short walkthrough of my early explorations.</p>

<h2>The data</h2>

<p>I&#39;ve been working a lot with broadcast media data.  One of my projects involves exploring the daily and weekly distribution of time spent on various topics and issues covered by news media.  </p>

<p>So let&#39;s say I have been estimating the proportion of </p>

<p>Here&#39;s an example of what this data looks like:</p>

<pre><code class="r">head(data_meta)
</code></pre>

<pre><code>##                                 X_id channel                          show
## 2 ObjectID(53514f1518d183146e0297df)    WBAL                 11_News_at_11
## 3 ObjectID(53514f1518d183146e0297e0)     KGO ABC_News_Good_Morning_America
## 4 ObjectID(53514f1518d183146e0297e1)   CSPAN                C-SPAN_Weekend
## 5 ObjectID(53514f1518d183146e0297e2)    CNNW                  CNN_Newsroom
## 6 ObjectID(53514f1518d183146e0297e3)    WETA                  Charlie_Rose
## 7 ObjectID(53514f1518d183146e0297e4)    WUSA              9News_Now_at_5am
##         date               travel              weather
## 2 2012-12-18                    0  0.24538968568110764
## 3 2013-07-20   0.1278605873919483 0.022781254955870685
## 4 2009-10-04 0.016729502805801842  0.01535445055974056
## 5 2013-05-01 0.050835327865678688                    0
## 6 2009-09-08                    0                    0
## 7 2012-05-30                    0  0.73114682970368883
##              courtcase              spanish        entertainment
## 2                    0                    0                    0
## 3  0.11708132091946018                    0 0.084127564947818062
## 4 0.014249124150100946 0.012178693872352059 0.028658802987892217
## 5                    0                    0                    0
## 6                    0                    0                    0
## 7                    0 0.024926795827934899                    0
##                anchors       international commercial (pharmaceuticals)
## 2                    0                   0                            0
## 3 0.053211951186896055                   0                            0
## 4 0.011049798871040071 0.51276068588080359         0.013694404795273802
## 5  0.28482916006481529                   0         0.090046909604937622
## 6                    0 0.14220227621188578                            0
## 7                    0                   0                            0
##                 sports                 food             disaster congress
## 2 0.051181164774435128                    0                    0        0
## 3 0.010236471895859771  0.42907115788935529 0.023480680939842322        0
## 4  0.01643453479315584 0.025048814439873255 0.016534022586565122        0
## 5                    0                    0                    0        0
## 6                    0                    0                    0        0
## 7                    0 0.023709207139960893                    0        0
##          international             election       international
## 2                    0                    0                   0
## 3                    0                    0                   0
## 4 0.020215145925370593 0.020380596933545568 0.01981606477389786
## 5                    0                    0                   0
## 6                    0                    0 0.64879509574938443
## 7                    0                    0                   0
##               congress              holiday               sports
## 2                    0  0.15938748235923789  0.12928395775625715
## 3                    0                    0 0.036021633565328566
## 4 0.021363046781977948 0.019380246931825981 0.023768516036536447
## 5                    0                    0                    0
## 6                    0                    0                    0
## 7 0.011866120100390988                    0 0.075958347079646174
##   commercial (healthcare)               noise                 guns
## 2                       0                   0  0.30624569555502507
## 3                       0                   0                    0
## 4    0.010302204760063252                   0 0.015419699258861708
## 5                       0                   0                    0
## 6                       0 0.20433971873521387                    0
## 7                       0                   0                    0
##              terrorism         gay marriage commericial (pharmaceuticals)
## 2                    0                    0                             0
## 3                    0                    0                             0
## 4 0.016154696635270641 0.014975569350437378           0.01272488145340281
## 5  0.50158505789980889                    0          0.033029030438272634
## 6                    0                    0                             0
## 7                    0 0.015530416954335324                             0
##              obamacare            campaigns             business
## 2                    0                    0                    0
## 3                    0                    0                    0
## 4 0.014950090339991593 0.013738158393050247 0.020096873662985959
## 5                    0                    0                    0
## 6                    0                    0                    0
## 7                    0 0.030038991037657305  0.03613208959912452
##                  local             congress              scandal
## 2 0.072404016072238328 0.032011391184895738                    0
## 3 0.035592987943989775                    0 0.053572183274267625
## 4 0.011747443301002086 0.021154305473626393 0.024581689768446522
## 5                    0                    0 0.038273122763922508
## 6                    0                    0                    0
## 7                    0                    0 0.037360094596050157
</code></pre>

<p>If I want to compute daily or weekly averages of the topic probability per show and per topic, I have to create a number of groupings according to those dimensions.  This is a classic split-apply-combine problem.  </p>

<p>To do weekly averages, this also requires a bit of date handling.  Luckily, <code>lubridate</code> makes this pretty easy (and I&#39;ve <a href="">written about <code>lubridate</code> before</a>).  What&#39;s the time period that I&#39;m evaluating?</p>

<pre><code class="r">start = min(data_meta$date)
end = max(data_meta$date)
start
</code></pre>

<pre><code>## [1] &quot;2009-06-04 UTC&quot;
</code></pre>

<pre><code class="r">end
</code></pre>

<pre><code>## [1] &quot;2014-03-25 UTC&quot;
</code></pre>

<p>How many days am I working with?</p>

<pre><code class="r">end - start
</code></pre>

<pre><code>## Time difference of 1755 days
</code></pre>

<p>How many weeks?</p>

<pre><code class="r">round((end - start)/eweeks(1))
</code></pre>

<pre><code>## [1] 251
</code></pre>

<p>Say I want to see a weekly average, but I want this to be over the full multi-year period.  I&#39;m going to create a new dimension that consists of a week index over this time period, and then I&#39;m going to transform this data from wide to long format.</p>

<pre><code class="r">data_meta$week = round((data_meta$date - min(data_meta$date))/eweeks(1))
data_meta$date = NULL  # I&#39;m going to melt this data and I don&#39;t want date in the way.
melted_data = melt(data_meta, id.vars = c(&quot;X_id&quot;, &quot;channel&quot;, &quot;show&quot;, &quot;week&quot;))
</code></pre>

<pre><code>## Warning: attributes are not identical across measure variables; they will
## be dropped
</code></pre>

<pre><code class="r">dim(melted_data)
</code></pre>

<pre><code>## [1] 10109525        6
</code></pre>

<h2>Creating a weekly average</h2>

<p>Again, this is a split-apply-combine problem.  I want to split this data into each topic, each channel, and each week.  For each of these subsets, I want to compute the average topic probability for each show&#39;s text that I&#39;ve got.  </p>

<p><code>plyr</code> was the canonical tool to turn to for split-apply-combine problems in <code>R</code>, and computing summary data within that analytical context is pretty textbook:</p>

<pre><code class="r">melted_data_total = ddply(melted_data, .(variable, channel, week), summarise, 
    value = mean(as.numeric(value), na.rm = T))
</code></pre>

<p>And this results in a weekly average of coverage per topic, such as the following:</p>

<pre><code class="r">dim(melted_data_total)
</code></pre>

<pre><code>## [1] 180225      4
</code></pre>

<pre><code class="r">head(melted_data_total)
</code></pre>

<pre><code>##   variable channel week   value
## 1   travel ALJAZAM  220 0.01298
## 2   travel ALJAZAM  221 0.02217
## 3   travel ALJAZAM  222 0.01152
## 4   travel ALJAZAM  223 0.01883
## 5   travel ALJAZAM  224 0.02175
## 6   travel ALJAZAM  225 0.02471
</code></pre>

<p>The problem is that this takes quite a bit of time to execute:</p>

<pre><code class="r">ddply_time = system.time(ddply(melted_data, .(variable, channel, week), summarise, 
    value = mean(as.numeric(value), na.rm = T)))
print(ddply_time)
</code></pre>

<pre><code>##    user  system elapsed 
##   435.2   255.0   690.2
</code></pre>

<p>This is a relatively small set of data compared to the real world. 400k observations is pretty reasonable and the scale of really Big Data, I can easily imagine wanting to explore millions of observations.</p>

<p><code>plyr</code> is wonderful, but one of the problems with this package (and with base <code>R</code> in general) is that it wasn&#39;t originally designed to scale to data of this size.  Nothing prevents you from running this <code>ddply</code> command, but what if you goofed in your selection of group-by variables?  For example, maybe I chose channel, but actually I meant show?  </p>

<p>It&#39;s completely reasonable to expect to have to iterate several times on a query to get the right subset and representation of your data.  And if a function takes minutes to hours to execute, it will slow down your workflow dramatically.</p>

<p><a href="http://blog.rstudio.org/2014/01/17/introducing-dplyr/">Enter <code>dplyr</code></a>.  I don&#39;t really want to go into the implementation details (and besides, you can always just <a href="https://github.com/hadley/dplyr">review the source code</a> if you really are curious).  There are really only a few things you need to know:</p>

<ol>
<li>If you are working with tabular data (or can think of your data as a <code>data.frame</code>), <code>dplyr</code> is for you</li>
<li>If you are frustrated with how slow it is to do split-apply-combine problems on your data with <code>ddply</code>, <code>dplyr</code> is for you</li>
<li>If you are working with data that is stored in a tabular format-supporting database (e.g. SQL), <code>dplyr</code> is for you.</li>
</ol>

<p>There are other tools out there for big <code>data.frames</code>, such as <code>data.table</code>, but I specifically wanted to do a head-to-head comparison of <code>dplyr</code> and <code>ddply</code>.  I like living in Wickham world.  One of my criticisms of <code>R</code> is that the community supports too many philosophies regarding how <code>R</code> packages should work, how syntax should be structured, and what kind of objects a package should create.  For me, <code>R</code> is about <code>data.frames</code>, and the Hadley world just feels like it&#39;s been more carefully considered.</p>

<h2>Get to the point.  How much faster is <code>dplyr</code>?</h2>

<p>Working with <code>dplyr</code> is pretty easy.  There&#39;s an operator <code>%.%</code> that allows for function chaining, which means that performing a split-apply-combine can be considered much in the same structure as <code>ddply</code>:</p>

<pre><code class="r">melted_data_dplyr = melted_data %.% group_by(variable, channel, week) %.% summarise(value = mean(as.numeric(value), 
    na.rm = T))

class(melted_data_dplyr)
</code></pre>

<pre><code>## [1] &quot;data.frame&quot;
</code></pre>

<p>You can treat the resulting object like a <code>data.frame</code> and call functions like <code>head</code> on it:</p>

<pre><code class="r">head(melted_data_dplyr)
</code></pre>

<pre><code>##     value
## 1 0.03143
</code></pre>

<p>We get the same results as with <code>ddply</code>.  So how much faster was the <code>dplyr</code> approach?</p>

<pre><code class="r">dplyr_time = system.time(melted_data %.% group_by(variable, channel, week) %.% 
    summarise(value = mean(as.numeric(value), na.rm = T)))
print(dplyr_time)
</code></pre>

<pre><code>##    user  system elapsed 
##   3.673   0.227   3.901
</code></pre>

<p>That&#39;s a pretty substantial increase in efficiency: from minutes to seconds.</p>

<pre><code class="r">ddply_time/dplyr_time
</code></pre>

<pre><code>##    user  system elapsed 
##   118.5  1123.4   176.9
</code></pre>

<p>That&#39;s about a 14-fold increase in efficiency.  Does this performance hold as data size increases?  Instead of a weekly average, let&#39;s try computing a daily average.  Going from 52 weeks to 365 days a year is about a 7-fold increase in data size, do we see a 7-fold increase in the execution time?</p>

<pre><code class="r">data_meta = cbind(meta, data)
melted_data = melt(data_meta, id.vars = c(&quot;X_id&quot;, &quot;channel&quot;, &quot;show&quot;, &quot;date&quot;))
</code></pre>

<pre><code>## Warning: attributes are not identical across measure variables; they will
## be dropped
</code></pre>

<pre><code class="r">
dplyr_time_daily = system.time(melted_data %.% group_by(variable, channel, date) %.% 
    summarise(value = mean(as.numeric(value), na.rm = T)))

melted_data_dplyr = melted_data %.% group_by(variable, channel, date) %.% summarise(value = mean(as.numeric(value), 
    na.rm = T))

dplyr_time_daily/dplyr_time
</code></pre>

<pre><code>##    user  system elapsed 
##   2.303   1.692   2.268
</code></pre>

<p>Looks like we see about a 4x increase in execution time going from weeks to days.  What&#39;s the dimensionality of these <code>data.frame</code>s and how long did it take to perform this <code>summarise</code>?</p>

<pre><code class="r">dim(melted_data)
</code></pre>

<pre><code>## [1] 10109525        6
</code></pre>

<pre><code class="r">tbl_df(melted_data_dplyr)
</code></pre>

<pre><code>## Source: local data frame [1 x 1]
## 
##     value
## 1 0.03143
</code></pre>

<pre><code class="r">dplyr_time_daily
</code></pre>

<pre><code>##    user  system elapsed 
##   8.460   0.384   8.848
</code></pre>

<p>About 22 seconds to work with over a million observations.  How long would this have taken with <code>ddply</code>?</p>

<pre><code class="r">ddply_time_daily = system.time(ddply(melted_data, .(variable, channel, date), 
    summarise, value = mean(as.numeric(value), na.rm = T)))
ddply_time_daily
</code></pre>

<pre><code>##    user  system elapsed 
##    2913    1608    4521
</code></pre>

<p>That&#39;s quite a bit longer in actual time; what is the rate of improvement?</p>

<pre><code class="r">ddply_time_daily/dplyr_time_daily
</code></pre>

<pre><code>##    user  system elapsed 
##   344.3  4187.5   511.0
</code></pre>

<p>Now we see about a 23-fold improvement using <code>dplyr</code> over <code>ddply</code>.  But because the data was in the scale of millions of rows, this means I had to wait about 8.5 minutes for the <code>ddply</code> transform to complete, whereas the <code>dplyr</code> version only took 20 seconds.  That means if I borked my initial transformation, I could probably try a few more versions of the same split-apply-combine approach in the amount of time it took to see a single result from <code>ddply</code>.</p>

<p>Additionally, it looks like the execution time of <code>ddply</code> more closely scales with the increase in observations.</p>

<pre><code class="r"># ddply_time_daily/ddply_time
</code></pre>

<h2>In summary&hellip;</h2>

<p><code>dplyr</code> is amazing.  This will dramatically increase my productivity with <code>R</code>.  I don&#39;t think I&#39;ll ever use <code>ddply</code> again.  I also prefer the expressiveness of the <code>%.%</code> operator (though I am honestly starting to grow a little concerned with all the operator overloading in the Wickham world of <code>R</code>).</p>

<h3>Version information</h3>

<pre><code class="r">sessionInfo()
</code></pre>

<pre><code>## R version 3.1.0 (2014-04-10)
## Platform: x86_64-apple-darwin10.8.0 (64-bit)
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] reshape2_1.4    lubridate_1.3.3 ggplot2_0.9.3.1 plyr_1.8.1     
## [5] dplyr_0.1.3     knitr_1.5      
## 
## loaded via a namespace (and not attached):
##  [1] assertthat_0.1   colorspace_1.2-4 digest_0.6.4     evaluate_0.5.5  
##  [5] formatR_0.10     grid_3.1.0       gtable_0.1.2     MASS_7.3-31     
##  [9] memoise_0.2.1    munsell_0.4.2    proto_0.3-10     Rcpp_0.11.1     
## [13] scales_0.2.4     stringr_0.6.2    tools_3.1.0
</code></pre>

</body>

</html>

